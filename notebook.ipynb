{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from typing import List, Union\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Credit Score Data Preprocessing Pipeline\n",
        "======================================\n",
        "This script handles comprehensive data cleaning, imputation, and feature engineering \n",
        "for credit scoring dataset with temporal customer data.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class CreditDataPreprocessor:\n",
        "    \"\"\"\n",
        "    A comprehensive data preprocessing pipeline for credit scoring data.\n",
        "\n",
        "    This class handles:\n",
        "    - Data cleaning and corruption removal\n",
        "    - Missing value imputation with customer-aware logic\n",
        "    - Feature engineering and encoding\n",
        "    - Outlier detection and treatment\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, file_path: str):\n",
        "        \"\"\"Initialize the preprocessor with data file.\"\"\"\n",
        "        self.file_path = file_path\n",
        "        self.df = None\n",
        "        self.setup_display_options()\n",
        "\n",
        "    def setup_display_options(self):\n",
        "        \"\"\"Configure pandas display options for better data inspection.\"\"\"\n",
        "        pd.set_option('display.max_columns', 1000)\n",
        "        pd.set_option('display.max_rows', 1000)\n",
        "        pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
        "\n",
        "    def load_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Load and perform initial data preparation.\"\"\"\n",
        "        print(\"Loading data...\")\n",
        "        self.df = pd.read_csv(self.file_path)\n",
        "        self.df.columns = self.df.columns.str.lower()\n",
        "        print(f\"Data loaded: {self.df.shape}\")\n",
        "        return self.df\n",
        "\n",
        "    def remove_irrelevant_columns(self) -> pd.DataFrame:\n",
        "        \"\"\"Remove columns that don't contribute to credit scoring.\"\"\"\n",
        "        columns_to_drop = ['id', 'name', 'ssn']\n",
        "        self.df = self.df.drop(columns_to_drop, axis=1)\n",
        "        print(f\"Dropped irrelevant columns: {columns_to_drop}\")\n",
        "        return self.df\n",
        "\n",
        "    def clean_corrupted_data(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Remove corrupted and invalid data patterns found in the dataset.\n",
        "\n",
        "        Handles:\n",
        "        - Extreme values wrapped in underscores\n",
        "        - Random character patterns\n",
        "        - Empty strings and various null representations\n",
        "        \"\"\"\n",
        "        print(\"Cleaning corrupted data...\")\n",
        "\n",
        "        # Handle specific extreme corrupted values\n",
        "        extreme_value_mapping = {\n",
        "            '__-333333333333333333333333333__': np.nan,\n",
        "            '__10000__': np.nan\n",
        "        }\n",
        "        self.df.replace(extreme_value_mapping, inplace=True)\n",
        "\n",
        "        # Define invalid patterns found in the data\n",
        "        invalid_patterns = ['', 'nan', '!@9#%8', '#F%$D@*&8', 'NM', 'nm']\n",
        "\n",
        "        # Strip underscores and replace invalid patterns\n",
        "        self.df = self.df.applymap(\n",
        "            lambda x: x if x is np.nan or not isinstance(x, str)\n",
        "            else str(x).strip('_')\n",
        "        ).replace(invalid_patterns, np.nan)\n",
        "\n",
        "        print(\"Corrupted data patterns cleaned\")\n",
        "        return self.df\n",
        "\n",
        "    def convert_data_types(self) -> pd.DataFrame:\n",
        "        \"\"\"Convert columns to appropriate data types after cleaning.\"\"\"\n",
        "        print(\"Converting data types...\")\n",
        "\n",
        "        # Numeric columns that should be converted\n",
        "        numeric_conversions = {\n",
        "            'age': int,\n",
        "            'annual_income': float,\n",
        "            'num_of_loan': int,\n",
        "            'num_of_delayed_payment': float,\n",
        "            'changed_credit_limit': float,\n",
        "            'outstanding_debt': float,\n",
        "            'amount_invested_monthly': float,\n",
        "            'monthly_balance': float\n",
        "        }\n",
        "\n",
        "        for col, dtype in numeric_conversions.items():\n",
        "            try:\n",
        "                self.df[col] = self.df[col].astype(dtype)\n",
        "            except ValueError as e:\n",
        "                print(f\"Warning: Could not convert {col} to {dtype}: {e}\")\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def standardize_string_columns(self) -> pd.DataFrame:\n",
        "        \"\"\"Standardize string columns to lowercase with underscores.\"\"\"\n",
        "        string_columns = list(self.df.dtypes[self.df.dtypes == 'object'].index)\n",
        "\n",
        "        for col in string_columns:\n",
        "            self.df[col] = self.df[col].str.lower().str.replace(' ', '_')\n",
        "\n",
        "        print(f\"Standardized {len(string_columns)} string columns\")\n",
        "        return self.df\n",
        "\n",
        "    def handle_customer_stable_features(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Handle features that should be stable within each customer's timeline.\n",
        "        Uses forward/backward fill within customer groups.\n",
        "        \"\"\"\n",
        "        print(\"Handling customer-stable features...\")\n",
        "\n",
        "        stable_features = ['occupation']\n",
        "\n",
        "        for feature in stable_features:\n",
        "            # Forward fill then backward fill within customer groups\n",
        "            self.df[feature] = self.df.groupby(\n",
        "                'customer_id')[feature].fillna(method='ffill')\n",
        "            self.df[feature] = self.df.groupby(\n",
        "                'customer_id')[feature].fillna(method='bfill')\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def clean_age_column(self) -> pd.DataFrame:\n",
        "        \"\"\"Clean age column with business logic validation.\"\"\"\n",
        "        print(\"Cleaning age column...\")\n",
        "\n",
        "        # Set unrealistic ages to NaN (based on data analysis: 14-60 range)\n",
        "        self.df.loc[(self.df['age'] < 14) | (\n",
        "            self.df['age'] > 60), 'age'] = np.nan\n",
        "\n",
        "        # Fill missing ages within customer groups\n",
        "        self.df['age'] = self.df.groupby('customer_id')['age'].fillna(\n",
        "            method='ffill').fillna(method='bfill')\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def parse_credit_history_age(self, x) -> Union[float, int]:\n",
        "        \"\"\"Parse credit history age from text format to months.\"\"\"\n",
        "        if pd.isna(x) or str(x).lower() == 'na':\n",
        "            return np.nan\n",
        "\n",
        "        parts = str(x).replace('_', ' ').split(' and ')\n",
        "        years = int(parts[0].split(' ')[0]) * 12\n",
        "        months = int(parts[1].split(' ')[0])\n",
        "\n",
        "        return years + months\n",
        "\n",
        "    def fill_credit_history_sequential(self, group) -> pd.Series:\n",
        "        \"\"\"\n",
        "        Fill missing credit history values considering monthly progression.\n",
        "        Credit history should increment by 1 each month.\n",
        "        \"\"\"\n",
        "        group = group.copy()\n",
        "\n",
        "        # Forward fill with increment\n",
        "        for i in range(1, len(group)):\n",
        "            if pd.isna(group.iloc[i]) and not pd.isna(group.iloc[i-1]):\n",
        "                group.iloc[i] = group.iloc[i-1] + 1\n",
        "\n",
        "        # Backward fill with decrement\n",
        "        for i in range(len(group)-2, -1, -1):\n",
        "            if pd.isna(group.iloc[i]) and not pd.isna(group.iloc[i+1]):\n",
        "                group.iloc[i] = group.iloc[i+1] - 1\n",
        "\n",
        "        return group\n",
        "\n",
        "    def handle_credit_history_age(self) -> pd.DataFrame:\n",
        "        \"\"\"Transform and impute credit history age column.\"\"\"\n",
        "        print(\"Processing credit history age...\")\n",
        "\n",
        "        # Replace 'na' strings with proper NaN\n",
        "        self.df['credit_history_age'] = self.df['credit_history_age'].replace({\n",
        "                                                                              'na': np.nan})\n",
        "\n",
        "        # Parse text format to numeric (months)\n",
        "        self.df['credit_history_age'] = self.df['credit_history_age'].apply(\n",
        "            self.parse_credit_history_age)\n",
        "\n",
        "        # Apply sequential filling within customer groups\n",
        "        self.df['credit_history_age'] = self.df.groupby('customer_id')['credit_history_age'].apply(\n",
        "            self.fill_credit_history_sequential\n",
        "        ).reset_index(level=0, drop=True)\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def impute_customer_grouped_features(self) -> pd.DataFrame:\n",
        "        \"\"\"Impute missing values using customer-grouped statistics.\"\"\"\n",
        "        print(\"Imputing customer-grouped features...\")\n",
        "\n",
        "        # Features that should be filled within customer groups\n",
        "        customer_grouped_features = {\n",
        "            'monthly_inhand_salary': 'ffill_bfill',\n",
        "            'credit_mix': 'ffill_bfill',\n",
        "            'payment_of_min_amount': 'mode',\n",
        "            'payment_behaviour': 'mode_safe',\n",
        "            'num_of_delayed_payment': 'median',\n",
        "            'changed_credit_limit': 'median'\n",
        "        }\n",
        "\n",
        "        for feature, method in customer_grouped_features.items():\n",
        "            if method == 'ffill_bfill':\n",
        "                self.df[feature] = self.df.groupby(\n",
        "                    'customer_id')[feature].fillna(method='ffill')\n",
        "                self.df[feature] = self.df.groupby(\n",
        "                    'customer_id')[feature].fillna(method='bfill')\n",
        "\n",
        "            elif method == 'mode':\n",
        "                self.df[feature] = self.df.groupby('customer_id')[feature].transform(\n",
        "                    lambda x: x.mode()[0] if not x.mode().empty else np.nan\n",
        "                )\n",
        "\n",
        "            elif method == 'mode_safe':\n",
        "                self.df[feature] = self.df.groupby('customer_id')[feature].transform(\n",
        "                    lambda x: x.fillna(\n",
        "                        x.mode()[0] if not x.mode().empty else 'unknown')\n",
        "                )\n",
        "\n",
        "            elif method == 'median':\n",
        "                self.df[feature] = self.df.groupby('customer_id')[feature].transform(\n",
        "                    lambda x: x.median() if not x.isnull().all() else np.nan\n",
        "                )\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def handle_remaining_missing_values(self) -> pd.DataFrame:\n",
        "        \"\"\"Handle remaining missing values with appropriate strategies.\"\"\"\n",
        "        print(\"Handling remaining missing values...\")\n",
        "\n",
        "        # Mean imputation for balance-related features\n",
        "        mean_imputation_cols = ['monthly_balance', 'amount_invested_monthly']\n",
        "        for col in mean_imputation_cols:\n",
        "            self.df[col] = self.df.groupby('customer_id')[col].transform(\n",
        "                lambda x: x.fillna(x.mean())\n",
        "            )\n",
        "\n",
        "        # Median imputation for count-based features with zero handling\n",
        "        median_imputation_cols = ['num_of_loan', 'num_credit_inquiries',\n",
        "                                  'num_bank_accounts', 'total_emi_per_month']\n",
        "\n",
        "        for col in median_imputation_cols:\n",
        "            # Customer-level median first\n",
        "            self.df[col] = self.df.groupby('customer_id')[col].transform(\n",
        "                lambda x: x.median() if not x.isnull().all() else np.nan\n",
        "            )\n",
        "\n",
        "            # Replace invalid zeros with NaN, then global median imputation\n",
        "            self.df[col] = self.df[col].replace(0, np.nan)\n",
        "            self.df[col].fillna(self.df[col].median(), inplace=True)\n",
        "\n",
        "            # Convert to integer for count-based features\n",
        "            if col in ['num_of_loan', 'num_credit_inquiries', 'num_bank_accounts']:\n",
        "                self.df[col] = self.df[col].astype(int)\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def engineer_loan_features(self) -> pd.DataFrame:\n",
        "        \"\"\"Create binary features for different loan types.\"\"\"\n",
        "        print(\"Engineering loan type features...\")\n",
        "\n",
        "        # Parse loan types and create binary features\n",
        "        loan_type_split = self.df['type_of_loan'].str.split(\n",
        "            r', and |, | and |,'\n",
        "        ).dropna()\n",
        "\n",
        "        # Extract all unique loan types\n",
        "        loan_types_list = [\n",
        "            item.removeprefix('_and_').strip('_')\n",
        "            for sublist in loan_type_split.tolist()\n",
        "            for item in sublist\n",
        "        ]\n",
        "        unique_loan_types = set(loan_types_list)\n",
        "\n",
        "        # Create binary features for each loan type\n",
        "        for loan_type in unique_loan_types:\n",
        "            if pd.notna(loan_type):\n",
        "                feature_name = f'has_{loan_type}'\n",
        "\n",
        "                # Check if customer has this loan type\n",
        "                self.df[feature_name] = self.df['type_of_loan'].apply(\n",
        "                    lambda x: int(loan_type in x) if pd.notna(x) else np.nan\n",
        "                )\n",
        "\n",
        "                # Fill missing values with mode\n",
        "                mode_value = self.df[feature_name].mode().iloc[0]\n",
        "                self.df[feature_name].fillna(mode_value, inplace=True)\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def create_dummy_variables(self) -> pd.DataFrame:\n",
        "        \"\"\"Create dummy variables for categorical features.\"\"\"\n",
        "        print(\"Creating dummy variables...\")\n",
        "\n",
        "        categorical_features = [\n",
        "            ('month', 'month'),\n",
        "            ('occupation', 'occupation'),\n",
        "            ('credit_mix', 'credit_mix'),\n",
        "            ('payment_of_min_amount', 'payment_of_min_amount'),\n",
        "            ('payment_behaviour', None)  # No prefix for payment_behaviour\n",
        "        ]\n",
        "\n",
        "        for feature, prefix in categorical_features:\n",
        "            if feature in self.df.columns:\n",
        "                if prefix:\n",
        "                    dummies = pd.get_dummies(\n",
        "                        self.df[feature],\n",
        "                        prefix=prefix,\n",
        "                        drop_first=True,\n",
        "                        dtype=int\n",
        "                    )\n",
        "                else:\n",
        "                    dummies = pd.get_dummies(\n",
        "                        self.df[feature],\n",
        "                        drop_first=True,\n",
        "                        dtype=int\n",
        "                    )\n",
        "\n",
        "                self.df = pd.concat([self.df, dummies], axis=1)\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def encode_target_variable(self) -> pd.DataFrame:\n",
        "        \"\"\"Encode the target variable (credit_score) to numeric values.\"\"\"\n",
        "        print(\"Encoding target variable...\")\n",
        "\n",
        "        # Map credit scores to numeric values (ordinal encoding)\n",
        "        credit_score_mapping = {\"poor\": 0, \"good\": 1, \"standard\": 2}\n",
        "        self.df['credit_score'] = self.df['credit_score'].replace(\n",
        "            credit_score_mapping)\n",
        "\n",
        "        print(\"Credit score distribution:\")\n",
        "        print(self.df['credit_score'].value_counts().sort_index())\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def drop_original_categorical_columns(self) -> pd.DataFrame:\n",
        "        \"\"\"Drop original categorical columns after creating dummy variables.\"\"\"\n",
        "        columns_to_drop = [\n",
        "            'type_of_loan', 'month', 'occupation', 'credit_mix',\n",
        "            'payment_of_min_amount', 'payment_behaviour', 'customer_id'\n",
        "        ]\n",
        "\n",
        "        existing_columns_to_drop = [\n",
        "            col for col in columns_to_drop if col in self.df.columns]\n",
        "        self.df = self.df.drop(existing_columns_to_drop, axis=1)\n",
        "\n",
        "        print(\n",
        "            f\"Dropped original categorical columns: {existing_columns_to_drop}\")\n",
        "        return self.df\n",
        "\n",
        "    def detect_and_treat_outliers(self, columns: List[str], method: str = 'iqr') -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Detect and treat outliers in specified columns.\n",
        "\n",
        "        Args:\n",
        "            columns: List of column names to check for outliers\n",
        "            method: Method for outlier detection ('iqr' for IQR method)\n",
        "        \"\"\"\n",
        "        print(f\"Treating outliers in {len(columns)} columns...\")\n",
        "\n",
        "        for col in columns:\n",
        "            if col not in self.df.columns:\n",
        "                continue\n",
        "\n",
        "            # Calculate IQR bounds\n",
        "            q1 = self.df[col].quantile(0.25)\n",
        "            q3 = self.df[col].quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "            lower_bound = q1 - 1.5 * iqr\n",
        "            upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "            # Count outliers\n",
        "            outliers_count = len(self.df[\n",
        "                (self.df[col] < lower_bound) | (self.df[col] > upper_bound)\n",
        "            ])\n",
        "\n",
        "            if outliers_count > 0:\n",
        "                print(f\"{col}: {outliers_count} outliers detected\")\n",
        "\n",
        "                # Treat outliers based on column characteristics\n",
        "                if col == 'total_emi_per_month':\n",
        "                    # Use median of non-outliers for EMI\n",
        "                    non_outlier_median = self.df[\n",
        "                        (self.df[col] >= lower_bound) & (\n",
        "                            self.df[col] <= upper_bound)\n",
        "                    ][col].median()\n",
        "\n",
        "                    self.df.loc[self.df[col] > upper_bound,\n",
        "                                col] = non_outlier_median\n",
        "\n",
        "                else:\n",
        "                    # Use customer-grouped mode for other features\n",
        "                    outlier_mask = (self.df[col] > upper_bound) | (\n",
        "                        self.df[col] < lower_bound)\n",
        "                    self.df.loc[outlier_mask, col] = self.df.groupby('customer_id')[col].transform(\n",
        "                        lambda x: x.mode()[0] if not x.mode(\n",
        "                        ).empty else x.median()\n",
        "                    )[outlier_mask]\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def generate_data_summary(self) -> pd.DataFrame:\n",
        "        \"\"\"Generate final data summary and statistics.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"DATA PREPROCESSING SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Final dataset shape: {self.df.shape}\")\n",
        "        print(f\"Missing values per column:\")\n",
        "        missing_values = self.df.isnull().sum()\n",
        "        if missing_values.sum() > 0:\n",
        "            print(missing_values[missing_values > 0])\n",
        "        else:\n",
        "            print(\"No missing values remaining!\")\n",
        "\n",
        "        print(f\"\\nData types:\")\n",
        "        print(self.df.dtypes.value_counts())\n",
        "\n",
        "        return self.df.describe()\n",
        "\n",
        "    def run_full_pipeline(self) -> pd.DataFrame:\n",
        "        \"\"\"Execute the complete preprocessing pipeline.\"\"\"\n",
        "        print(\"Starting Credit Score Data Preprocessing Pipeline...\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Load and initial cleaning\n",
        "        self.load_data()\n",
        "        self.remove_irrelevant_columns()\n",
        "        self.clean_corrupted_data()\n",
        "        self.convert_data_types()\n",
        "        self.standardize_string_columns()\n",
        "\n",
        "        # Handle missing values with domain knowledge\n",
        "        self.handle_customer_stable_features()\n",
        "        self.clean_age_column()\n",
        "        self.handle_credit_history_age()\n",
        "        self.impute_customer_grouped_features()\n",
        "        self.handle_remaining_missing_values()\n",
        "\n",
        "        # Feature engineering\n",
        "        self.engineer_loan_features()\n",
        "        self.create_dummy_variables()\n",
        "        self.encode_target_variable()\n",
        "\n",
        "        # Outlier treatment for key numerical columns\n",
        "        numerical_columns = [\n",
        "            'num_credit_card', 'interest_rate', 'num_credit_inquiries',\n",
        "            'annual_income', 'total_emi_per_month'\n",
        "        ]\n",
        "        self.detect_and_treat_outliers(numerical_columns)\n",
        "        \n",
        "        # Final cleanup\n",
        "        self.drop_original_categorical_columns()\n",
        "        \n",
        "        # Generate summary\n",
        "        summary = self.generate_data_summary()\n",
        "\n",
        "        print(\"\\nPreprocessing pipeline completed successfully!\")\n",
        "        return self.df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Credit Score Data Preprocessing Pipeline...\n",
            "============================================================\n",
            "Loading data...\n",
            "Data loaded: (100000, 28)\n",
            "Dropped irrelevant columns: ['id', 'name', 'ssn']\n",
            "Cleaning corrupted data...\n",
            "Corrupted data patterns cleaned\n",
            "Converting data types...\n",
            "Standardized 9 string columns\n",
            "Handling customer-stable features...\n",
            "Cleaning age column...\n",
            "Processing credit history age...\n",
            "Imputing customer-grouped features...\n",
            "Handling remaining missing values...\n",
            "Engineering loan type features...\n",
            "Creating dummy variables...\n",
            "Encoding target variable...\n",
            "Credit score distribution:\n",
            "credit_score\n",
            "0    28998\n",
            "1    17828\n",
            "2    53174\n",
            "Name: count, dtype: int64\n",
            "Treating outliers in 5 columns...\n",
            "num_credit_card: 2271 outliers detected\n",
            "interest_rate: 2034 outliers detected\n",
            "num_credit_inquiries: 696 outliers detected\n",
            "annual_income: 2783 outliers detected\n",
            "total_emi_per_month: 6200 outliers detected\n",
            "Dropped original categorical columns: ['type_of_loan', 'month', 'occupation', 'credit_mix', 'payment_of_min_amount', 'payment_behaviour', 'customer_id']\n",
            "\n",
            "==================================================\n",
            "DATA PREPROCESSING SUMMARY\n",
            "==================================================\n",
            "Final dataset shape: (100000, 56)\n",
            "Missing values per column:\n",
            "No missing values remaining!\n",
            "\n",
            "Data types:\n",
            "int32      32\n",
            "float64    20\n",
            "int64       4\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Preprocessing pipeline completed successfully!\n",
            "Processed data saved to 'processed_credit_data.csv'\n"
          ]
        }
      ],
      "source": [
        "# Initialize and run preprocessing pipeline\n",
        "preprocessor = CreditDataPreprocessor(\"./train.csv\")\n",
        "processed_df = preprocessor.run_full_pipeline()\n",
        "# Optional: Save processed data\n",
        "processed_df.to_csv(\"processed_credit_data.csv\", index=False)\n",
        "print(\"Processed data saved to 'processed_credit_data.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score,make_scorer, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = processed_df.drop('credit_score', axis=1)\n",
        "y = processed_df['credit_score']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "integer_columns = ['age',\n",
        "                   'num_bank_accounts',\n",
        "                   'num_credit_card',\n",
        "                   'interest_rate',\n",
        "                   'num_of_loan',\n",
        "                   'delay_from_due_date',\n",
        "                   'num_of_delayed_payment',\n",
        "                   'num_credit_inquiries',\n",
        "                   'credit_history_age']\n",
        "\n",
        "float_columns = ['credit_utilization_ratio',\n",
        "                 'annual_income',\n",
        "                 'monthly_inhand_salary',\n",
        "                 'changed_credit_limit',\n",
        "                 'outstanding_debt',\n",
        "                 'total_emi_per_month',\n",
        "                 'amount_invested_monthly',\n",
        "                 'monthly_balance']\n",
        "\n",
        "numerical_columns = integer_columns + float_columns\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
        "X_test[numerical_columns] = scaler.transform(X_test[numerical_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"XGBoost\": XGBClassifier()\n",
        "}\n",
        "\n",
        "params = {\n",
        "    \"Random Forest\": {\n",
        "        'n_estimators': [16, 32, 64, 128],\n",
        "        'max_depth': [None, 10, 20],\n",
        "       # 'min_samples_split': [5, 10],\n",
        "        #'min_samples_leaf': [2, 5],\n",
        "        #'bootstrap': [True, False],\n",
        "    },\n",
        "    \"XGBoost\": {\n",
        "        'learning_rate': [0.1, 0.05, 0.001],\n",
        "        'n_estimators': [16, 32, 64, 128],\n",
        "        #'max_depth': [3, 4, 5],\n",
        "        'subsample': [0.7, 0.8]\n",
        "    }\n",
        "}\n",
        "\n",
        "f1_scorer = make_scorer(f1_score, average='weighted')\n",
        "\n",
        "grid_searches = {}\n",
        "for model_name, model in models.items():\n",
        "    grid_search = GridSearchCV(\n",
        "        model,\n",
        "        params[model_name],\n",
        "        cv=3,  # Use the number of desired cross-validation folds\n",
        "        scoring=f1_scorer,\n",
        "        n_jobs=-1,  # Use all available CPU cores\n",
        "        verbose=2,\n",
        "    )\n",
        "    grid_searches[model_name] = grid_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Random Forest': GridSearchCV(cv=3, estimator=RandomForestClassifier(), n_jobs=-1,\n",
              "              param_grid={'max_depth': [None, 10, 20],\n",
              "                          'n_estimators': [16, 32, 64, 128]},\n",
              "              scoring=make_scorer(f1_score, response_method='predict', average=weighted),\n",
              "              verbose=2),\n",
              " 'XGBoost': GridSearchCV(cv=3,\n",
              "              estimator=XGBClassifier(base_score=None, booster=None,\n",
              "                                      callbacks=None, colsample_bylevel=None,\n",
              "                                      colsample_bynode=None,\n",
              "                                      colsample_bytree=None, device=None,\n",
              "                                      early_stopping_rounds=None,\n",
              "                                      enable_categorical=False, eval_metric=None,\n",
              "                                      feature_types=None, feature_weights=None,\n",
              "                                      gamma=None, grow_policy=None,\n",
              "                                      importance_type=None,\n",
              "                                      interaction_constraints=Non...\n",
              "                                      max_delta_step=None, max_depth=None,\n",
              "                                      max_leaves=None, min_child_weight=None,\n",
              "                                      missing=nan, monotone_constraints=None,\n",
              "                                      multi_strategy=None, n_estimators=None,\n",
              "                                      n_jobs=None, num_parallel_tree=None, ...),\n",
              "              n_jobs=-1,\n",
              "              param_grid={'learning_rate': [0.1, 0.05, 0.001],\n",
              "                          'n_estimators': [16, 32, 64, 128],\n",
              "                          'subsample': [0.7, 0.8]},\n",
              "              scoring=make_scorer(f1_score, response_method='predict', average=weighted),\n",
              "              verbose=2)}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_searches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m best_models = {}\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, grid_search \u001b[38;5;129;01min\u001b[39;00m grid_searches.items():\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# X_train and y_train are your training data\u001b[39;00m\n\u001b[32m      4\u001b[39m     best_models[model_name] = grid_search.best_estimator_\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1569\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1570\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1571\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1760\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1761\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1763\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "best_models = {}\n",
        "for model_name, grid_search in grid_searches.items():\n",
        "    grid_search.fit(X_train, y_train)  # X_train and y_train are your training data\n",
        "    best_models[model_name] = grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_f1_score = -1  # Initialize with a low value\n",
        "best_model = None\n",
        "\n",
        "for model_name, grid_search in grid_searches.items():\n",
        "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
        "    print(f\"Best F1-score for {model_name}: {grid_search.best_score_}\")\n",
        "    print(\"==\"*25,\"\\n\")\n",
        "\n",
        "    if grid_search.best_score_ > best_f1_score:\n",
        "        best_f1_score = grid_search.best_score_\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "if best_model is not None:\n",
        "    print(\"Best model based on F1-score:\")\n",
        "    print(best_model)\n",
        "    print(f\"Best F1-score: {best_f1_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(best_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = best_model.predict(X_test)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Plot a heatmap for the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(f'Confusion Matrix for {best_model}')\n",
        "plt.show()\n",
        "\n",
        "# Print the classification report\n",
        "print(f\"Classification Report for {best_model}:\\n\")\n",
        "print(report)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
